{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe8833d-c5b8-4668-9ff8-ead4478a3e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col, to_date\n",
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import countDistinct,count\n",
    "from pyspark.sql.functions import max ,min\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.sql.functions as F \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, date_sub, dayofweek\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.functions import col, date_format, date_sub, expr\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql.functions import date_trunc\n",
    "from pyspark.sql.functions import concat_ws,col\n",
    "from pyspark.sql.functions import col, to_date\n",
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import countDistinct,count\n",
    "from pyspark.sql.functions import max ,min\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from scipy import stats\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from scipy.stats import chisquare\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import regexp_replace, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2017de-cbd7-400f-b7c6-da9e849209fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a001947-17f2-4543-b54d-106b94cfd734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5cebbba-91c1-4ae3-871e-65901aa6d5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21109f8-6365-4445-b949-a84edbca6c91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## RX Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fbb8d1-e9ee-43ab-948d-ac90b4dddfc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "rx_rank = spark.read.table(f'dsf_hackathon_2024.rx_rank')\n",
    "rx_rank.display()\n",
    "rx_rank.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1bb0d9c-092f-45ba-9098-f0cbe749733f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as sum_\n",
    " \n",
    "df = rx_rank  \n",
    " \n",
    "df_sorted = df.orderBy(col(\"pct\").desc())\n",
    "\n",
    "df_top_10_ndc = df_sorted.limit(10)\n",
    "\n",
    "# Display the result\n",
    "\n",
    "df_top_10_ndc.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38dbeaf6-599c-4127-bbb3-216a642d763f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PX Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f380a41-4d6b-4678-a091-69c52af10452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "px_rank = spark.read.table(f'dsf_hackathon_2024.px_rank')\n",
    "px_rank.display()\n",
    "px_rank.count()\n",
    "\n",
    "\n",
    "df = px_rank   \n",
    "df_sorted = df.orderBy(col(\"pct\").desc())\n",
    "df_top_10 = df_sorted.filter(col(\"pct\") <= 100).limit(10)\n",
    "\n",
    "\n",
    "df_top_10.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d18c777-a450-45fd-a8ff-8e9def9cd134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Diagnosis Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511563b2-75f8-44bc-9868-ad472586b351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "diagnosis_codes = spark.read.table(f'dsf_hackathon_2024.dx_rank')\n",
    "diagnosis_codes.display()\n",
    "diagnosis_codes.select('diagnosis_code').distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a29e54f-4118-4c89-9497-709c616dc468",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diagnosis_codes_top_10 = diagnosis_codes.orderBy(col(\"pct\").desc()).filter(col(\"pct\") >= 10).limit(10)\n",
    "diagnosis_codes_top_10.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4f05ff-e263-46af-9c0b-1edffeb6e026",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Labs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192a1bf9-bab2-4714-b134-d2440c8d5a60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "labs_data = spark.read.table(f'dsf_hackathon_2024.labs_play')\n",
    "data_sample_labs_data = labs_data\n",
    "data_sample_labs_data.display()\n",
    "data_sample_labs_data.select('lab_test_loinc_cd').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b830e3ba-c32e-4ee0-a977-d7489e86fa9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587ff792-4fe1-4973-99bc-de15a341678e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "demog = spark.read.table(f'dsf_hackathon_2024.master_play')\n",
    "demog.display()\n",
    "demog.groupby('hiv').agg(F.count('id')).display()\n",
    "data_sample_demog = demog\n",
    "data_sample_demog.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dda3ccd-8c41-473a-b667-f568f3902228",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, max as max_, first\n",
    "\n",
    "df = data_sample_demog\n",
    " \n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = [\"gender\",\"region\",\"race\"]\n",
    " \n",
    "# List of date columns (where we need to take the max date)\n",
    "date_columns = []\n",
    " \n",
    "# List of columns that are already numeric (0/1) or other types\n",
    "numeric_columns = ['age']\n",
    " \n",
    "# Track the dummy columns created\n",
    "dummy_columns = []\n",
    " \n",
    "# One-hot encode each categorical column\n",
    "for column in columns_to_encode:\n",
    "    distinct_values_df = df.select(column).distinct()\n",
    "    for row in distinct_values_df.collect():\n",
    "        value = row[column]\n",
    "        encoded_col_name = f\"{column}_{value}\"\n",
    "        dummy_columns.append(encoded_col_name)  # Track the new dummy column\n",
    "        df = df.withColumn(encoded_col_name, when(col(column) == value, 1).otherwise(0))\n",
    " \n",
    "\n",
    "aggregation_exprs = []\n",
    " \n",
    "# Aggregate dummy columns using max\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in dummy_columns])\n",
    " \n",
    "# Aggregate date columns using max (to get the most recent date)\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in date_columns])\n",
    " \n",
    "# For numeric columns (0/1), we can use max or sum depending on the context\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in numeric_columns])\n",
    " \n",
    "# If there are other columns where you want to keep the first non-null value, you can do:\n",
    "# aggregation_exprs.extend([first(col, ignorenulls=True).alias(col) for col in other_columns])\n",
    " \n",
    "# Group by patient_id == id, hiv and apply the aggregations\n",
    "df_data_sample_demog = df.groupBy(\"id\",'hiv').agg(*aggregation_exprs)\n",
    " \n",
    "df_data_sample_demog.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f987b973-bb9f-4354-bdc9-9b2163608de2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Master Play Control Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503e8042-8ba0-4a9c-abb9-9d3ace1dc506",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "master_play_control = spark.read.table(f'dsf_hackathon_2024.master_play_control_partition')\n",
    "master_play_control.display()\n",
    "master_play_control.select('id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2245f813-a0f9-47bf-9512-2d02b5b1e82d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_play_control_top_10 = master_play_control.orderBy(col(\"pr\").desc()).filter(col(\"pr\") >= 90).limit(10)\n",
    "master_play_control_top_10.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8238f80f-3691-4236-95b2-d3497f4f1790",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Master Play HIV (Test) Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15821a5-e35b-4940-b7c2-604d24bdac8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "master_play_hiv = spark.read.table(f'dsf_hackathon_2024.master_play_hiv_partition')\n",
    "master_play_hiv.display()\n",
    "master_play_hiv.select('id').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5bbf0b7-1171-4239-b5ae-ff46acb56d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Medical Claims Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc1d4a3-effc-479b-b9fd-426e08fdc2d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "medical_claims_play = spark.read.table(f'dsf_hackathon_2024.medical_claims_play')\n",
    "medical_claims_play.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457bfdd7-143f-475a-9389-ee91dcbb03a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df1_diagnosis_codes = [row['diagnosis_code'] for row in diagnosis_codes_top_10.select(\"diagnosis_code\").distinct().collect()]\n",
    "\n",
    "medical_claims_play_with_top_10 = medical_claims_play.withColumn(\n",
    "    \"new_diagnosis_code\",\n",
    "    when(col(\"diagnosis_code\").isin(df1_diagnosis_codes), col(\"diagnosis_code\")).otherwise(\"others\")\n",
    ")\n",
    "\n",
    "display(medical_claims_play_with_top_10)\n",
    "\n",
    "df1_procedure_codes = [row['procedure_code'] for row in df_top_10.select(\"procedure_code\").distinct().collect()]\n",
    "medical_claims_play_with_top_10 = medical_claims_play_with_top_10.withColumn(\n",
    "    \"new_procedure_code\",\n",
    "    when(col(\"procedure_code\").isin(df1_procedure_codes), col(\"procedure_code\")).otherwise(\"others\")\n",
    ")\n",
    "\n",
    "display(medical_claims_play_with_top_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2baeddf1-c435-40ec-92e2-86782abd6345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, max as max_, first\n",
    "\n",
    "df = medical_claims_play_with_top_10\n",
    " \n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = [\"new_diagnosis_code\", \"new_procedure_code\"]\n",
    " \n",
    "# List of date columns (where we need to take the max date)\n",
    "date_columns = []\n",
    " \n",
    "# List of columns that are already numeric (0/1) or other types\n",
    "numeric_columns = []\n",
    " \n",
    "# Track the dummy columns created\n",
    "dummy_columns = []\n",
    " \n",
    "# One-hot encode each categorical column\n",
    "for column in columns_to_encode:\n",
    "    distinct_values_df = df.select(column).distinct()\n",
    "    for row in distinct_values_df.collect():\n",
    "        value = row[column]\n",
    "        encoded_col_name = f\"{column}_{value}\"\n",
    "        dummy_columns.append(encoded_col_name)  # Track the new dummy column\n",
    "        df = df.withColumn(encoded_col_name, when(col(column) == value, 1).otherwise(0))\n",
    " \n",
    "\n",
    "aggregation_exprs = []\n",
    " \n",
    "# Aggregate dummy columns using max\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in dummy_columns])\n",
    " \n",
    "# Aggregate date columns using max (to get the most recent date)\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in date_columns])\n",
    " \n",
    "# For numeric columns (0/1), we can use max or sum depending on the context\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in numeric_columns])\n",
    " \n",
    "# If there are other columns where you want to keep the first non-null value, you can do:\n",
    "# aggregation_exprs.extend([first(col, ignorenulls=True).alias(col) for col in other_columns])\n",
    " \n",
    "# Group by patient_id == id, hiv and apply the aggregations\n",
    "df_grouped_master_claims = df.groupBy(\"id\",'hiv').agg(*aggregation_exprs)\n",
    " \n",
    "df_grouped_master_claims.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c0ddd2-7363-416d-b1de-f14900c60d35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pharmacy Claims Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ffb50ab-5d32-4bf5-9a46-b7ec2faa9bf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "pharmacy_claims_play = spark.read.table(f'dsf_hackathon_2024.pharmacy_claims_play')\n",
    "pharmacy_claims_play_sample = pharmacy_claims_play\n",
    "pharmacy_claims_play_sample.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90971d28-150d-40a3-833a-6f95486d6601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df1_ndc_codes = [row['ndc_code'] for row in df_top_10_ndc.select(\"ndc_code\").distinct().collect()]\n",
    "\n",
    "pharmacy_claims_play_with_top_10 = pharmacy_claims_play.withColumn(\n",
    "    \"new_ndc_code\",\n",
    "    when(col(\"ndc_code\").isin(df1_ndc_codes), col(\"ndc_code\")).otherwise(\"others\")\n",
    ")\n",
    "\n",
    "display(pharmacy_claims_play_with_top_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff37a8f8-8076-410c-8bc4-250fd3607367",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, max as max_, first\n",
    "\n",
    "df = pharmacy_claims_play_with_top_10\n",
    " \n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = [\"new_ndc_code\"]\n",
    " \n",
    "# List of date columns (where we need to take the max date)\n",
    "date_columns = []\n",
    " \n",
    "# List of columns that are already numeric (0/1) or other types\n",
    "numeric_columns = []\n",
    " \n",
    "# Track the dummy columns created\n",
    "dummy_columns = []\n",
    " \n",
    "# One-hot encode each categorical column\n",
    "for column in columns_to_encode:\n",
    "    distinct_values_df = df.select(column).distinct()\n",
    "    for row in distinct_values_df.collect():\n",
    "        value = row[column]\n",
    "        encoded_col_name = f\"{column}_{value}\"\n",
    "        dummy_columns.append(encoded_col_name)  # Track the new dummy column\n",
    "        df = df.withColumn(encoded_col_name, when(col(column) == value, 1).otherwise(0))\n",
    " \n",
    "\n",
    "aggregation_exprs = []\n",
    " \n",
    "# Aggregate dummy columns using max\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in dummy_columns])\n",
    " \n",
    "# Aggregate date columns using max (to get the most recent date)\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in date_columns])\n",
    " \n",
    "# For numeric columns (0/1), we can use max or sum depending on the context\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in numeric_columns])\n",
    " \n",
    "# If there are other columns where you want to keep the first non-null value, you can do:\n",
    "#aggregation_exprs.extend([first(col, ignorenulls=True).alias(col) for col in other_columns])\n",
    " \n",
    "# Group by patient_id == id, hiv and apply the aggregations\n",
    "df_grouped_pharmacy_claims_play = df.groupBy(\"id\",'hiv').agg(*aggregation_exprs)\n",
    " \n",
    "df_grouped_pharmacy_claims_play.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ceb945-7f3a-4101-bea6-85e6e742112d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Master Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "887aec4e-2915-4351-8d18-bede78e88457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Master Data Combined\n",
    "\n",
    "join_key = \"id\" \n",
    " \n",
    "# Perform the left join between df1 and df2\n",
    "df1_df2_joined = df_data_sample_demog.join(\n",
    "    df_grouped_master_claims.select(join_key, *[col for col in df_grouped_master_claims.columns if col not in df_data_sample_demog.columns]),\n",
    "    on=join_key,\n",
    "    how=\"left\"\n",
    ")\n",
    " \n",
    "# Now, join the result with df3\n",
    "final_df_sample = df1_df2_joined.join(\n",
    "    df_grouped_pharmacy_claims_play.select(join_key, *[col for col in df_grouped_pharmacy_claims_play.columns if col not in df1_df2_joined.columns]),\n",
    "    on=join_key,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final_df_sample.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e35d82-c20c-4126-9539-f029ec7ca856",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_with_age_groups = final_df_sample.withColumn(\n",
    "    \"age_20_30\", \n",
    "    when((col(\"age\") >= 20) & (col(\"age\") < 30), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_30_40\", \n",
    "    when((col(\"age\") >= 30) & (col(\"age\") < 40), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_40_50\", \n",
    "    when((col(\"age\") >= 40) & (col(\"age\") < 50), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_50_60\", \n",
    "    when((col(\"age\") >= 50) & (col(\"age\") < 60), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_60_70\", \n",
    "    when((col(\"age\") >= 60) & (col(\"age\") < 70), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_grater_than_70\", \n",
    "    when((col(\"age\") >= 70) & (col(\"age\") < 100), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df_with_age_groups.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615b90f3-78da-4964-b217-90e710ba1b11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_model_data = df_with_age_groups.drop('id','age').fillna(0)\n",
    "\n",
    "master_model_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca68c76f-53a9-4540-9f3b-b815a8d92adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_model_data.groupby('hiv').agg(F.count('hiv')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70509bd8-9216-44b5-8e1f-fd10fb06fe57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = master_model_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70123ca-4c48-4277-a9f6-09ab9be3145a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model using RF and XGBoost Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894708e2-9481-4186-83ec-cdc58779b0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Separate features and target variable\n",
    "X = pandas_df.drop(columns=[\"hiv\"])\n",
    "y = pandas_df[\"hiv\"]\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the data into training and testing sets (70:30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    " \n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['auto']\n",
    "}\n",
    " \n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [10],\n",
    "    'learning_rate': [0.1],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    " \n",
    "# Initialize the models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    " \n",
    "# Perform GridSearchCV to find the best parameters for Random Forest\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2, scoring='f1')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    " \n",
    "# Perform GridSearchCV to find the best parameters for XGBoost\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, verbose=2, scoring='f1')\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    " \n",
    "# Get the best parameters and best models\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_model_rf = grid_search_rf.best_estimator_\n",
    " \n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_model_xgb = grid_search_xgb.best_estimator_\n",
    " \n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_rf)\n",
    "print(\"Best Hyperparameters for XGBoost:\", best_params_xgb)\n",
    " \n",
    "# Train the models with the best parameters\n",
    "best_model_rf.fit(X_train, y_train)\n",
    "best_model_xgb.fit(X_train, y_train)\n",
    " \n",
    "# Predict on the test set\n",
    "y_pred_rf = best_model_rf.predict(X_test)\n",
    "y_pred_proba_rf = best_model_rf.predict_proba(X_test)[:, 1]\n",
    " \n",
    "y_pred_xgb = best_model_xgb.predict(X_test)\n",
    "y_pred_proba_xgb = best_model_xgb.predict_proba(X_test)[:, 1]\n",
    " \n",
    "# Calculate F1 Score\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    " \n",
    "# Calculate ROC AUC\n",
    "roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    " \n",
    "# Get feature importances\n",
    "feature_importances_rf = best_model_rf.feature_importances_\n",
    "feature_importances_xgb = best_model_xgb.feature_importances_\n",
    "\n",
    "# Create DataFrames for feature importances\n",
    "features_df_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances_rf\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    " \n",
    "features_df_xgb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances_xgb\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "# Plot feature importances for Random Forest\n",
    "plt.figure(figsize=(10, 16))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=features_df_rf, palette=\"viridis\")\n",
    "plt.title(\"Feature Importance - Random Forest\")\n",
    "plt.show()\n",
    " \n",
    "# Plot feature importances for XGBoost\n",
    "plt.figure(figsize=(10, 16))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=features_df_xgb, palette=\"viridis\")\n",
    "plt.title(\"Feature Importance - XGBoost\")\n",
    "plt.show()\n",
    " \n",
    "# Plot ROC Curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "roc_auc_value_rf = auc(fpr_rf, tpr_rf)\n",
    " \n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "roc_auc_value_xgb = auc(fpr_xgb, tpr_xgb)\n",
    " \n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label=f'Random Forest ROC curve (area = {roc_auc_value_rf:.2f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='blue', lw=2, label=f'XGBoost ROC curve (area = {roc_auc_value_xgb:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    " \n",
    "# Evaluate models on training and test sets\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    " \n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    " \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred)\n",
    "    train_recall = recall_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    " \n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    " \n",
    "    print(\"\\nTrain Set Metrics:\")\n",
    "    print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Precision: {train_precision:.4f}\")\n",
    "    print(f\"Recall: {train_recall:.4f}\")\n",
    "    print(f\"F1 Score: {train_f1:.4f}\")\n",
    "    print(f\"ROC AUC Score: {train_roc_auc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    " \n",
    "    print(\"\\nTest Set Metrics:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    print(f\"F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"ROC AUC Score: {test_roc_auc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    " \n",
    "# Evaluate Random Forest\n",
    "print(\"Random Forest Evaluation:\")\n",
    "evaluate_model(best_model_rf, X_train, y_train, X_test, y_test)\n",
    " \n",
    "# Evaluate XGBoost\n",
    "print(\"XGBoost Evaluation:\")\n",
    "evaluate_model(best_model_xgb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c483bd-0839-4e3a-99e8-514936747f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Holdout set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c58f74-3bbf-4908-b0fb-63199c0288b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "labs_test = spark.read.table(f'dsf_hackathon_2024_holdout_participant.labs_test')\n",
    "labs_test.display()\n",
    "labs_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eab11f8-8a5b-4d2a-a9af-57201ad91c11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "master_test = spark.read.table(f'dsf_hackathon_2024_holdout_participant.master_test')\n",
    "data_test_demog = master_test\n",
    "data_test_demog.display()\n",
    "data_test_demog.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d732ca55-38fc-4def-a7aa-a807e6c3fadb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, max as max_, first\n",
    "\n",
    "df = data_test_demog\n",
    " \n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = [\"gender\",\"region\",\"race\"]\n",
    " \n",
    "# List of date columns (where we need to take the max date)\n",
    "date_columns = []\n",
    " \n",
    "# List of columns that are already numeric (0/1) or other types\n",
    "numeric_columns = ['age']\n",
    " \n",
    "# Track the dummy columns created\n",
    "dummy_columns = []\n",
    " \n",
    "# One-hot encode each categorical column\n",
    "for column in columns_to_encode:\n",
    "    distinct_values_df = df.select(column).distinct()\n",
    "    for row in distinct_values_df.collect():\n",
    "        value = row[column]\n",
    "        encoded_col_name = f\"{column}_{value}\"\n",
    "        dummy_columns.append(encoded_col_name)  # Track the new dummy column\n",
    "        df = df.withColumn(encoded_col_name, when(col(column) == value, 1).otherwise(0))\n",
    " \n",
    "\n",
    "aggregation_exprs = []\n",
    " \n",
    "# Aggregate dummy columns using max\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in dummy_columns])\n",
    " \n",
    "# Aggregate date columns using max (to get the most recent date)\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in date_columns])\n",
    " \n",
    "# For numeric columns (0/1), we can use max or sum depending on the context\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in numeric_columns])\n",
    " \n",
    "# If there are other columns where you want to keep the first non-null value, you can do:\n",
    "# aggregation_exprs.extend([first(col, ignorenulls=True).alias(col) for col in other_columns])\n",
    " \n",
    "# Group by patient_id == id, hiv and apply the aggregations\n",
    "df_data_test_demog = df.groupBy(\"id\").agg(*aggregation_exprs)\n",
    " \n",
    "df_data_test_demog.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0774d267-e60c-4478-a1a6-14d41bdbd98b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "medical_claims_test = spark.read.table(f'dsf_hackathon_2024_holdout_participant.medical_claims_test')\n",
    "medical_claims_test.display()\n",
    "medical_claims_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebf7c0f-c607-4d34-94dd-c61e09ef61a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df1_diagnosis_codes = [row['diagnosis_code'] for row in diagnosis_codes_top_10.select(\"diagnosis_code\").distinct().collect()]\n",
    "\n",
    "medical_claims_test_with_top_10 = medical_claims_test.withColumn(\n",
    "    \"new_diagnosis_code\",\n",
    "    when(col(\"diagnosis_code\").isin(df1_diagnosis_codes), col(\"diagnosis_code\")).otherwise(\"others\")\n",
    ")\n",
    "\n",
    "display(medical_claims_test_with_top_10)\n",
    "\n",
    "df1_procedure_codes = [row['procedure_code'] for row in df_top_10.select(\"procedure_code\").distinct().collect()]\n",
    "medical_claims_test_with_top_10 = medical_claims_test_with_top_10.withColumn(\n",
    "    \"new_procedure_code\",\n",
    "    when(col(\"procedure_code\").isin(df1_procedure_codes), col(\"procedure_code\")).otherwise(\"others\")\n",
    ")\n",
    "\n",
    "display(medical_claims_test_with_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "debeebcf-3770-45e4-b018-4b56c14c6baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, max as max_, first\n",
    "\n",
    "df = medical_claims_test_with_top_10\n",
    " \n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = [\"new_diagnosis_code\", \"new_procedure_code\"]\n",
    " \n",
    "# List of date columns (where we need to take the max date)\n",
    "date_columns = []\n",
    " \n",
    "# List of columns that are already numeric (0/1) or other types\n",
    "numeric_columns = []\n",
    " \n",
    "# Track the dummy columns created\n",
    "dummy_columns = []\n",
    " \n",
    "# One-hot encode each categorical column\n",
    "for column in columns_to_encode:\n",
    "    distinct_values_df = df.select(column).distinct()\n",
    "    for row in distinct_values_df.collect():\n",
    "        value = row[column]\n",
    "        encoded_col_name = f\"{column}_{value}\"\n",
    "        dummy_columns.append(encoded_col_name)  # Track the new dummy column\n",
    "        df = df.withColumn(encoded_col_name, when(col(column) == value, 1).otherwise(0))\n",
    " \n",
    "\n",
    "aggregation_exprs = []\n",
    " \n",
    "# Aggregate dummy columns using max\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in dummy_columns])\n",
    " \n",
    "# Aggregate date columns using max (to get the most recent date)\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in date_columns])\n",
    " \n",
    "# For numeric columns (0/1), we can use max or sum depending on the context\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in numeric_columns])\n",
    " \n",
    "# If there are other columns where you want to keep the first non-null value, you can do:\n",
    "# aggregation_exprs.extend([first(col, ignorenulls=True).alias(col) for col in other_columns])\n",
    " \n",
    "# Group by patient_id == id, hiv and apply the aggregations\n",
    "df_grouped_test_master_claims = df.groupBy(\"id\").agg(*aggregation_exprs)\n",
    " \n",
    "df_grouped_test_master_claims.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65154d9e-989e-46d3-a7a3-f3c3f6c2ba34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_NAME = '`dsf-hackathon-hackathon-a-dev`'\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "pharmacy_claims_test = spark.read.table(f'dsf_hackathon_2024_holdout_participant.pharmacy_claims_test')\n",
    "pharmacy_claims_test.display()\n",
    "pharmacy_claims_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526f13bb-0073-4af5-a60e-9da66275de17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df1_ndc_codes = [row['ndc_code'] for row in df_top_10_ndc.select(\"ndc_code\").distinct().collect()]\n",
    "\n",
    "pharmacy_claims_test_with_top_10 = pharmacy_claims_test.withColumn(\n",
    "    \"new_ndc_code\",\n",
    "    when(col(\"ndc_code\").isin(df1_ndc_codes), col(\"ndc_code\")).otherwise(\"others\")\n",
    ")\n",
    "\n",
    "display(pharmacy_claims_test_with_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d124c2-89f2-4579-9ec5-ef5133f65f3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, max as max_, first\n",
    "\n",
    "df = pharmacy_claims_test_with_top_10\n",
    " \n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = [\"new_ndc_code\"]\n",
    " \n",
    "# List of date columns (where we need to take the max date)\n",
    "date_columns = []\n",
    " \n",
    "# List of columns that are already numeric (0/1) or other types\n",
    "numeric_columns = []\n",
    " \n",
    "# Track the dummy columns created\n",
    "dummy_columns = []\n",
    " \n",
    "# One-hot encode each categorical column\n",
    "for column in columns_to_encode:\n",
    "    distinct_values_df = df.select(column).distinct()\n",
    "    for row in distinct_values_df.collect():\n",
    "        value = row[column]\n",
    "        encoded_col_name = f\"{column}_{value}\"\n",
    "        dummy_columns.append(encoded_col_name)  # Track the new dummy column\n",
    "        df = df.withColumn(encoded_col_name, when(col(column) == value, 1).otherwise(0))\n",
    " \n",
    "\n",
    "aggregation_exprs = []\n",
    " \n",
    "# Aggregate dummy columns using max\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in dummy_columns])\n",
    " \n",
    "# Aggregate date columns using max (to get the most recent date)\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in date_columns])\n",
    " \n",
    "# For numeric columns (0/1), we can use max or sum depending on the context\n",
    "aggregation_exprs.extend([max_(col).alias(col) for col in numeric_columns])\n",
    " \n",
    "# If there are other columns where you want to keep the first non-null value, you can do:\n",
    "#aggregation_exprs.extend([first(col, ignorenulls=True).alias(col) for col in other_columns])\n",
    " \n",
    "# Group by patient_id == id, hiv and apply the aggregations\n",
    "df_grouped_pharmacy_claims_test = df.groupBy(\"id\").agg(*aggregation_exprs)\n",
    " \n",
    "df_grouped_pharmacy_claims_test.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1d028d-e676-4275-a2a9-664c8a6fbe87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Master Data Combined\n",
    "\n",
    "join_key = \"id\" \n",
    " \n",
    "# Perform the left join between df1 and df2\n",
    "df1_df2_joined_test = df_data_test_demog.join(\n",
    "    df_grouped_test_master_claims.select(join_key, *[col for col in df_grouped_test_master_claims.columns if col not in df_data_test_demog.columns]),\n",
    "    on=join_key,\n",
    "    how=\"left\"\n",
    ")\n",
    " \n",
    "# Now, join the result with df3\n",
    "final_df_test = df1_df2_joined_test.join(\n",
    "    df_grouped_pharmacy_claims_test.select(join_key, *[col for col in df_grouped_pharmacy_claims_test.columns if col not in df1_df2_joined_test.columns]),\n",
    "    on=join_key,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final_df_test.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32a9b37-8d5e-40c6-a271-1e33ab355cd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e930557-a3ed-4e55-88cb-0ede821cbad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_test_with_age_groups = final_df_test.withColumn(\n",
    "    \"age_20_30\", \n",
    "    when((col(\"age\") >= 20) & (col(\"age\") < 30), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_30_40\", \n",
    "    when((col(\"age\") >= 30) & (col(\"age\") < 40), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_40_50\", \n",
    "    when((col(\"age\") >= 40) & (col(\"age\") < 50), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_50_60\", \n",
    "    when((col(\"age\") >= 50) & (col(\"age\") < 60), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_60_70\", \n",
    "    when((col(\"age\") >= 60) & (col(\"age\") < 70), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"age_grater_than_70\", \n",
    "    when((col(\"age\") >= 70) & (col(\"age\") < 100), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df_test_with_age_groups.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37019a7-7d65-4954-bdec-cba75b0fd364",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_model_data_test = df_test_with_age_groups.drop('id','age').fillna(0)\n",
    "\n",
    "pandas_df_test = master_model_data_test.toPandas()\n",
    "display(pandas_df_test)\n",
    "len(pandas_df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea21d0c-ead1-4ccf-a79e-86c77ed99f9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred_rf = best_model_rf.predict(pandas_df_test)\n",
    "y_pred_proba_rf = best_model_rf.predict_proba(pandas_df_test)\n",
    " \n",
    "y_pred_xgb = best_model_xgb.predict(pandas_df_test)\n",
    "y_pred_proba_xgb = best_model_xgb.predict_proba(pandas_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848650c4-a9f3-4cdd-98d5-96bdbd1a6061",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df_test['y_pred_rf'] = y_pred_rf\n",
    "pandas_df_test['y_pred_proba_rf'] = y_pred_proba_rf[:,1]\n",
    "pandas_df_test['y_pred_xgb'] = y_pred_xgb\n",
    "pandas_df_test['y_pred_proba_xgb'] = y_pred_proba_xgb[:,1]\n",
    "\n",
    "display(pandas_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6746d8-c9e6-46a4-ad74-cd292356a2c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Final_test_with_predictions = df_test_with_age_groups.toPandas()\n",
    "\n",
    "Final_test_with_predictions['y_pred_rf'] = y_pred_rf\n",
    "Final_test_with_predictions['y_pred_proba_rf'] = y_pred_proba_rf[:,1]\n",
    "Final_test_with_predictions['y_pred_xgb'] = y_pred_xgb\n",
    "Final_test_with_predictions['y_pred_proba_xgb'] = y_pred_proba_xgb[:,1]\n",
    "\n",
    "\n",
    "display(Final_test_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933e1566-46e1-4ce5-81d3-fcefd6f9a093",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "Final_test_with_predictions['timestamp'] = datetime.now()\n",
    "Final_test_with_predictions = Final_test_with_predictions[['id', 'y_pred_rf', 'y_pred_proba_rf', 'timestamp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3bfd4a-400f-40b7-8301-82052cb5d026",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Final_test_with_predictions = Final_test_with_predictions.rename(columns={'y_pred_rf': 'prediction', 'y_pred_proba_rf': 'positive_class_prob'})\n",
    "display(Final_test_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbc7f4e-fc98-4da7-bdb3-665518d72cee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scoring_dataset_hackathon_a_2024_08_30_10_46_47 = spark.createDataFrame(Final_test_with_predictions)\n",
    "\n",
    "# 2024-08-30T10:46:47.263+00:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8a99a5-c635-4216-a528-bc765e4e09cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scoring_dataset_hackathon_a_2024_08_30_10_46_47.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248d80ed-d442-4e76-a544-6afffcf07f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scoring_dataset_hackathon_a_2024_08_30_10_46_47.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85c395f-4a93-45be-8aa3-12c305242843",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the catalog, schema, and table name\n",
    "catalog_name = \"`dsf-hackathon-hackathon-a-dev`\"  # Replace with your catalog name\n",
    "schema_name = \"`dsf-hackathon-scoring`\"    # Replace with your schema name\n",
    "table_name = \"scoring_dataset_hackathon_a_2024_08_30_10_46_47\"      # Replace with your table name\n",
    "\n",
    "# Write the DataFrame as a table in the specified catalog and schema\n",
    "scoring_dataset_hackathon_a_2024_08_30_10_46_47.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hackathon 2024 Model Final",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
